{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishee82oo/nfs-oil-price-prediction/blob/main/GDELT_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXr_gIiBYl3U",
        "outputId": "bd01d607-132b-4dc5-8de2-9e28fd8d30ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage pyarrow pycountry tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C8_bX21FYp5P"
      },
      "outputs": [],
      "source": [
        "import importlib.util, sys, os, io, json, gzip\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd, numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "48mIZtkIYrIa"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except Exception:\n",
        "    pass\n",
        "import google.auth\n",
        "from google.cloud import storage\n",
        "credentials, default_project = google.auth.default()\n",
        "client = storage.Client(project=default_project, credentials=credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uPZ2jLAZIk8"
      },
      "source": [
        "# Import data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XaZWM8XTYs7j"
      },
      "outputs": [],
      "source": [
        "spec = importlib.util.spec_from_file_location(\"gdelt_module\", \"/mnt/data/gdelt_data_engineering_clean.ipynb\")\n",
        "gdelt_module = None\n",
        "if spec and spec.loader:\n",
        "    gm = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(gm)\n",
        "    gdelt_module = gm\n",
        "if gdelt_module and hasattr(gdelt_module, \"GCS_BUCKET_NAME\"):\n",
        "    BUCKET_NAME = getattr(gdelt_module, \"GCS_BUCKET_NAME\")\n",
        "else:\n",
        "    BUCKET_NAME = os.environ.get(\"GCS_BUCKET_NAME\", \"gdelt_raw_3_years\")\n",
        "if gdelt_module and hasattr(gdelt_module, \"GCS_PROCESSED_PATH\"):\n",
        "    PROCESSED_PREFIX = getattr(gdelt_module, \"GCS_PROCESSED_PATH\")\n",
        "else:\n",
        "    PROCESSED_PREFIX = os.environ.get(\"GCS_PROCESSED_PATH\", \"processed_data/\")\n",
        "bucket = client.bucket(BUCKET_NAME)\n",
        "blobs = list(client.list_blobs(BUCKET_NAME, prefix=PROCESSED_PREFIX))\n",
        "final_blobs = [b for b in blobs if b.name.startswith(f\"{PROCESSED_PREFIX}final_aligned_data_\") and b.name.endswith(\".json.gz\")]\n",
        "final_blobs_sorted = sorted(final_blobs, key=lambda b: b.name, reverse=True)\n",
        "if len(final_blobs_sorted)==0:\n",
        "    raise SystemExit(\"No final_aligned_data_*.json.gz found under the processed prefix\")\n",
        "latest_blob = final_blobs_sorted[0]\n",
        "local_download_path = \"/tmp/latest_final_aligned_data.json.gz\"\n",
        "with open(local_download_path, \"wb\") as f:\n",
        "    f.write(latest_blob.download_as_bytes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyfaKsH_ZPWj"
      },
      "source": [
        "# Forming Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4tTv_JeHPeD",
        "outputId": "4804bca7-7421-4f4c-a2ab-e44e2cc37bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded: /tmp/latest_final_aligned_data.json.gz\n",
            "Size: 2.37 MB\n",
            "Downloaded from GCS: processed_data/final_aligned_data_20250908.json.gz\n",
            "Bucket: gdelt_raw_3_years\n",
            "Last modified: 2025-09-08 01:30:57.993000+00:00\n",
            "Data preview:\n",
            "[{\"date\": \"20220825\", \"country\": \"US\", \"event_count\": 2301, \"avg_sentiment\": -0.023938528465884453, \"unique_sources\": 728, \"wti_price\": 93.33, \"brent_price\": 98.81, \"theme_energy\": 45, \"theme_conflict...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(local_download_path):\n",
        "    file_size = os.path.getsize(local_download_path) / (1024*1024)\n",
        "    print(f\"File downloaded: {local_download_path}\")\n",
        "    print(f\"Size: {file_size:.2f} MB\")\n",
        "\n",
        "else:\n",
        "    print(\"File NOT found locally\")\n",
        "\n",
        "print(f\"Downloaded from GCS: {latest_blob.name}\")\n",
        "print(f\"Bucket: {BUCKET_NAME}\")\n",
        "print(f\"Last modified: {latest_blob.updated}\")\n",
        "\n",
        "with gzip.open(local_download_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "    sample = f.read(500)\n",
        "    print(f\"Data preview:\\n{sample[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AseuuqqNYuqr",
        "outputId": "8d036973-93a6-4da8-ee02-4f42d0313db2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143401, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "with gzip.open(local_download_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "records = json.loads(raw)\n",
        "df = pd.DataFrame.from_records(records)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYBXmOXTYx7t",
        "outputId": "17300291-bd1e-4488-fae3-bf3b1c244387"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143401, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import pycountry\n",
        "def to_iso3(name):\n",
        "    try:\n",
        "        c = pycountry.countries.lookup(name)\n",
        "        return c.alpha_3\n",
        "    except Exception:\n",
        "        s = str(name).upper()\n",
        "        s2 = \"\".join([c for c in s if c.isalpha() or c==\" \"]).strip().replace(\" \", \"_\")\n",
        "        return s2\n",
        "df[\"country_iso3\"] = df[\"country\"].fillna(\"UNKNOWN\").apply(to_iso3)\n",
        "df[\"node_id\"] = df[\"country_iso3\"].astype(str) + \"_\" + df[\"date\"].dt.strftime(\"%Y%m%d\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qFVQHJNdYyka"
      },
      "outputs": [],
      "source": [
        "price_by_date = df[[\"date\",\"wti_price\"]].drop_duplicates().set_index(\"date\").sort_index()\n",
        "price_by_date[\"wti_price\"] = pd.to_numeric(price_by_date[\"wti_price\"], errors=\"coerce\")\n",
        "price_by_date = price_by_date.sort_index()\n",
        "price_by_date[\"wti_price_next\"] = price_by_date[\"wti_price\"].shift(-1)\n",
        "price_by_date[\"wti_delta_next\"] = price_by_date[\"wti_price_next\"] - price_by_date[\"wti_price\"]\n",
        "price_by_date[\"wti_ret_next\"] = price_by_date[\"wti_delta_next\"] / price_by_date[\"wti_price\"]\n",
        "price_map = price_by_date.to_dict(orient=\"index\")\n",
        "def attach_targets(row):\n",
        "    pdx = row[\"date\"]\n",
        "    v = price_map.get(pdx)\n",
        "    if v is None:\n",
        "        return pd.Series([np.nan,np.nan])\n",
        "    return pd.Series([v.get(\"wti_delta_next\"), v.get(\"wti_ret_next\")])\n",
        "df[[\"wti_delta_next\",\"wti_ret_next\"]] = df.apply(attach_targets, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "price_lag_features = price_by_date[['wti_price']].copy()\n",
        "\n",
        "price_lag_features['wti_price_lag_1'] = price_lag_features['wti_price'].shift(1)\n",
        "price_lag_features['wti_price_lag_2'] = price_lag_features['wti_price'].shift(2)\n",
        "\n",
        "price_lag_features['wti_delta_prev_1'] = price_lag_features['wti_price'] - price_lag_features['wti_price_lag_1']\n",
        "price_lag_features['wti_delta_prev_3'] = price_lag_features['wti_price'] - price_lag_features['wti_price'].shift(3)\n",
        "\n",
        "price_lag_features['wti_ret_prev_1'] = price_lag_features['wti_delta_prev_1'] / price_lag_features['wti_price_lag_1'].replace(0, np.nan)\n",
        "price_lag_features['wti_ret_prev_3'] = price_lag_features['wti_price'].div(price_lag_features['wti_price'].shift(3)) - 1\n",
        "\n",
        "price_lag_features['wti_rolling_mean_3'] = price_lag_features['wti_price'].rolling(window=3, min_periods=1).mean()\n",
        "price_lag_features['wti_rolling_std_3'] = price_lag_features['wti_price'].rolling(window=3, min_periods=2).std()\n",
        "price_lag_features['wti_rolling_mean_7'] = price_lag_features['wti_price'].rolling(window=7, min_periods=1).mean()\n",
        "price_lag_features['wti_rolling_std_7'] = price_lag_features['wti_price'].rolling(window=7, min_periods=2).std()\n",
        "\n",
        "price_lag_features = price_lag_features.drop(columns=['wti_price'])\n",
        "lag_feature_cols = list(price_lag_features.columns)"
      ],
      "metadata": {
        "id": "AZ5_vgz4kl1G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cZL1fJFZY0Li"
      },
      "outputs": [],
      "source": [
        "feature_cols = [c for c in df.columns if c not in [\"country\",\"date\",\"node_id\",\"country_iso3\",\"wti_delta_next\",\"wti_ret_next\"]]\n",
        "feature_cols = [c for c in feature_cols if df[c].dtype != \"object\" or c.startswith(\"theme_\")]\n",
        "node_features = df[[\"node_id\",\"country\",\"country_iso3\",\"date\",\"wti_price\",\"brent_price\",\"wti_delta_next\",\"wti_ret_next\"] + [c for c in df.columns if c in feature_cols]]\n",
        "node_features = node_features.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mN4YlVMFY2Hv"
      },
      "outputs": [],
      "source": [
        "opec_members = [\"Venezuela\",\"Saudi Arabia\",\"Iran\",\"Iraq\",\"Kuwait\",\"UAE\",\"Qatar\",\"Algeria\",\"Angola\",\"Libya\",\"Nigeria\",\"Ecuador\",\"Gabon\",\"Republic of the Congo\",\"Equatorial Guinea\"]\n",
        "iso_map = {}\n",
        "for name in opec_members:\n",
        "    try:\n",
        "        iso_map[name] = pycountry.countries.lookup(name).alpha_3\n",
        "    except Exception:\n",
        "        iso_map[name] = name.upper().replace(\" \",\"_\")\n",
        "opec_iso = list(iso_map.values())\n",
        "from itertools import combinations\n",
        "static_edges = []\n",
        "for a,b in combinations(opec_iso,2):\n",
        "    static_edges.append({\"source\":a,\"target\":b,\"edge_type\":\"opec_member\"})\n",
        "static_edges_df = pd.DataFrame(static_edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ccoFJtmnY4gH"
      },
      "outputs": [],
      "source": [
        "dyn_edges_df = pd.DataFrame(columns=[\"source\",\"target\",\"edge_type\",\"timestamp\",\"date\"])\n",
        "potential_actor_cols = [c for c in df.columns if \"actor\" in c.lower() or \"actor1\" in c.lower() or \"actor2\" in c.lower()]\n",
        "if len(potential_actor_cols)>0:\n",
        "    actors = []\n",
        "    for idx,row in df.iterrows():\n",
        "        for k in potential_actor_cols:\n",
        "            v = row.get(k)\n",
        "        if row.get(\"actor1\") and row.get(\"actor2\"):\n",
        "            s = str(row.get(\"actor1\"))\n",
        "            t = str(row.get(\"actor2\"))\n",
        "            dyn_edges_df.loc[len(dyn_edges_df)] = [s,t,\"gdelt_event\",row.get(\"date\"),row.get(\"date\")]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HYnQXhhiY4SS"
      },
      "outputs": [],
      "source": [
        "df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in [\"country\",\"date\",\"node_id\",\"country_iso3\",\"wti_delta_next\",\"wti_ret_next\",\"wti_price\",\"brent_price\"]]\n",
        "feature_cols = [c for c in feature_cols if df[c].dtype != \"object\" or c.startswith(\"theme_\")]\n",
        "node_features = df[[\"node_id\",\"country\",\"country_iso3\",\"date\",\"wti_price\",\"brent_price\",\"wti_delta_next\",\"wti_ret_next\"] + [c for c in df.columns if c in feature_cols]]\n",
        "node_features = node_features.loc[:, ~node_features.columns.duplicated()]\n",
        "node_features = node_features.merge(price_lag_features, left_on='date', right_index=True, how='left')\n",
        "node_features = node_features.fillna(0)\n",
        "\n",
        "feature_cols = list(dict.fromkeys(feature_cols + lag_feature_cols))\n",
        "\n",
        "out_dir_local = \"/tmp/graph_export\"\n",
        "os.makedirs(out_dir_local, exist_ok=True)\n",
        "nodes_out = os.path.join(out_dir_local, \"nodes.parquet\")\n",
        "static_edges_out = os.path.join(out_dir_local, \"edges_static.parquet\")\n",
        "dyn_edges_out = os.path.join(out_dir_local, \"edges_dynamic.parquet\")\n",
        "node_features.to_parquet(nodes_out, index=False)\n",
        "static_edges_df.to_parquet(static_edges_out, index=False)\n",
        "dyn_edges_df.to_parquet(dyn_edges_out, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4YQp6wWY4Ee",
        "outputId": "6ea40be9-20bb-4547-f2f1-aa70ba95d539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3841086253.py:7: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  meta = {\"nodes\":nodes_out,\"edges_static\":static_edges_out,\"edges_dynamic\":dyn_edges_out,\"uploaded_at\":datetime.utcnow().isoformat()}\n"
          ]
        }
      ],
      "source": [
        "gcs_prefix = PROCESSED_PREFIX + \"graph_dataset/\"\n",
        "for p in [nodes_out, static_edges_out, dyn_edges_out]:\n",
        "    bn = os.path.basename(p)\n",
        "    blob = bucket.blob(f\"{gcs_prefix}{bn}\")\n",
        "    with open(p,\"rb\") as f:\n",
        "        blob.upload_from_file(f)\n",
        "meta = {\"nodes\":nodes_out,\"edges_static\":static_edges_out,\"edges_dynamic\":dyn_edges_out,\"uploaded_at\":datetime.utcnow().isoformat()}\n",
        "meta_blob = bucket.blob(f\"{gcs_prefix}metadata.json\")\n",
        "meta_blob.upload_from_string(json.dumps(meta), content_type=\"application/json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9RNhXaqY-7y",
        "outputId": "027a2a00-ef45-4112-b212-235fdcfc7cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 733/733 [09:04<00:00,  1.35it/s]\n"
          ]
        }
      ],
      "source": [
        "unique_dates = sorted(node_features[\"date\"].dt.date.unique())\n",
        "for d in tqdm(unique_dates):\n",
        "    sub = node_features[node_features[\"date\"].dt.date==d].copy()\n",
        "    if sub.shape[0]==0:\n",
        "        continue\n",
        "    fn = f\"node_features_{d.strftime('%Y%m%d')}.parquet\"\n",
        "    localp = os.path.join(out_dir_local,fn)\n",
        "    sub.to_parquet(localp, index=False)\n",
        "    blob = bucket.blob(f\"{gcs_prefix}{fn}\")\n",
        "    with open(localp,\"rb\") as f:\n",
        "        blob.upload_from_file(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruFEh4nmY-5c",
        "outputId": "23257b64-0a20-432c-9ad8-ca102c72dd5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "bucket gdelt_raw_3_years\n",
            "processed prefix processed_data/\n",
            "latest aligned blob processed_data/final_aligned_data_20250908.json.gz\n",
            "graph files uploaded to processed_data/graph_dataset/\n"
          ]
        }
      ],
      "source": [
        "print(\"done\")\n",
        "print(\"bucket\",BUCKET_NAME)\n",
        "print(\"processed prefix\",PROCESSED_PREFIX)\n",
        "print(\"latest aligned blob\", latest_blob.name)\n",
        "print(\"graph files uploaded to\", PROCESSED_PREFIX + \"graph_dataset/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YT_peZp17pp"
      },
      "source": [
        "# Baseline Model: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4L1H1N5B6tO",
        "outputId": "314e40bc-dbc3-4ddb-fa81-5afbf96cc273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "zwvfkp7G2IiE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EC2QStHI2bej"
      },
      "outputs": [],
      "source": [
        "modeling_df = node_features.copy()\n",
        "target_col = 'wti_delta_next'\n",
        "modeling_df = modeling_df.dropna(subset=[target_col]).sort_values('date').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JtIVi6yJ6m9D"
      },
      "outputs": [],
      "source": [
        "X_all = modeling_df[feature_cols].fillna(0.0).values\n",
        "y_all = modeling_df[target_col].astype(float).values\n",
        "\n",
        "if modeling_df['date'].nunique() > 30:\n",
        "    cutoff_date = modeling_df['date'].max() - pd.Timedelta(days=90)\n",
        "    train_mask = modeling_df['date'] < cutoff_date\n",
        "    if train_mask.sum() == 0 or train_mask.sum() == len(modeling_df):\n",
        "        train_mask = np.arange(len(modeling_df)) < int(0.8 * len(modeling_df))\n",
        "else:\n",
        "    train_mask = np.arange(len(modeling_df)) < int(0.8 * len(modeling_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o8FRuNEj7Lo4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_all[train_mask], X_all[~train_mask]\n",
        "y_train, y_test = y_all[train_mask], y_all[~train_mask]\n",
        "\n",
        "if len(y_test) == 0:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_all, y_all, test_size=0.2, random_state=42, shuffle=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "do9sspl7-6OU"
      },
      "outputs": [],
      "source": [
        "xgb = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [200, 300, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'reg_lambda': [0.1, 1.0, 5.0]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cwCUSWBc_Cxb"
      },
      "outputs": [],
      "source": [
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# 4. Set up the GridSearchCV\n",
        "# We use 'r2' as the scoring metric to optimize for.\n",
        "# n_jobs=-1 uses all available CPU cores to speed up the search.\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='r2',\n",
        "    cv=tscv,  # <--- Use TimeSeriesSplit here!\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf7VgM8T_Jye",
        "outputId": "3abe7687-deb6-4c75-e32e-a9ea9a7c8e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
            "Best R-squared score found: -0.0187\n",
            "Best parameters found:\n",
            "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_lambda': 0.1, 'subsample': 0.8}\n"
          ]
        }
      ],
      "source": [
        "grid_search.fit(X_all, y_all)\n",
        "\n",
        "print(f\"Best R-squared score found: {grid_search.best_score_:.4f}\")\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnRdm_tu8ew4",
        "outputId": "aa83587b-2128-4d10-9e0c-b87e7bb89312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test metrics:\n",
            "  MAE: 0.9734\n",
            "  RMSE: 1.5267\n",
            "  R2: 0.3095\n"
          ]
        }
      ],
      "source": [
        "model = grid_search.best_estimator_\n",
        "y_pred = model.predict(X_test)\n",
        "metrics = {\n",
        "    'MAE': mean_absolute_error(y_test, y_pred),\n",
        "    'RMSE': mean_squared_error(y_test, y_pred),\n",
        "    'R2': r2_score(y_test, y_pred)\n",
        "}\n",
        "print('Test metrics:')\n",
        "for name, value in metrics.items():\n",
        "    print(f'  {name}: {value:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9rgHaE69zoL"
      },
      "source": [
        "# GNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqoVIXH5pCWq",
        "outputId": "caf56e14-a3a4-4689-97e6-44207590ba7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torch-geometric -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NyPbB10u93Fm"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xJ4YznVWu_yk"
      },
      "outputs": [],
      "source": [
        "node_features['date'] = pd.to_datetime(node_features['date'])\n",
        "price_by_date.index = pd.to_datetime(price_by_date.index)\n",
        "if not dyn_edges_df.empty:\n",
        "    dyn_edges_df['date'] = pd.to_datetime(dyn_edges_df['date'], errors='coerce')\n",
        "    if 'timestamp' in dyn_edges_df.columns:\n",
        "        dyn_edges_df['timestamp'] = pd.to_datetime(dyn_edges_df['timestamp'], errors='coerce')\n",
        "    else:\n",
        "        dyn_edges_df['timestamp'] = dyn_edges_df['date']\n",
        "    dyn_edges_df['source_iso3'] = dyn_edges_df['source'].apply(to_iso3)\n",
        "    dyn_edges_df['target_iso3'] = dyn_edges_df['target'].apply(to_iso3)\n",
        "    dyn_edges_df = dyn_edges_df.dropna(subset=['source_iso3','target_iso3'])\n",
        "else:\n",
        "    dyn_edges_df['timestamp'] = pd.NaT\n",
        "    dyn_edges_df['source_iso3'] = []\n",
        "    dyn_edges_df['target_iso3'] = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KnLB4ULa5SI",
        "outputId": "4f651982-efb1-4652-90a7-35d7a1b6e9b3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143401, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "J9t7UT33955M"
      },
      "outputs": [],
      "source": [
        "feature_columns_graph = [c for c in node_features.columns if c not in ['node_id','country','country_iso3','date','wti_price','brent_price','wti_delta_next','wti_ret_next']]\n",
        "feature_columns_graph = [c for c in feature_columns_graph if node_features[c].dtype != 'object']\n",
        "feature_columns_graph = sorted(dict.fromkeys(feature_columns_graph))\n",
        "\n",
        "market_indicator_candidates = []\n",
        "for col in node_features.columns:\n",
        "    low = col.lower()\n",
        "    if any(key in low for key in ['spread','usd','inventory','fx','dxy','dollar','inflation']):\n",
        "        if node_features[col].dtype != 'object':\n",
        "            market_indicator_candidates.append(col)\n",
        "market_indicator_cols = ['wti_price','brent_price'] + sorted(dict.fromkeys(market_indicator_candidates))\n",
        "engineered_feature_cols = [c for c in lag_feature_cols if c in node_features.columns]\n",
        "engineered_feature_cols = sorted(dict.fromkeys(engineered_feature_cols))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GraphSnapshot:\n",
        "    date: pd.Timestamp\n",
        "    node_features: np.ndarray\n",
        "    adjacency: np.ndarray\n",
        "    engineered_features: np.ndarray\n",
        "    target: float\n",
        "\n",
        "def _normalize_adjacency(adj: np.ndarray) -> np.ndarray:\n",
        "    deg = adj.sum(axis=1, keepdims=True)\n",
        "    deg[deg == 0] = 1.0\n",
        "    return adj / deg\n",
        "\n",
        "def build_adjacency_matrix(nodes_df: pd.DataFrame, static_edges_df: pd.DataFrame, dyn_edges_df: pd.DataFrame, snapshot_time: pd.Timestamp, lookback_hours: int = 24) -> np.ndarray:\n",
        "    n = len(nodes_df)\n",
        "    adjacency = np.zeros((n, n), dtype=float)\n",
        "    iso_to_indices = defaultdict(list)\n",
        "    for idx, iso in enumerate(nodes_df['country_iso3']):\n",
        "        iso_to_indices[iso].append(idx)\n",
        "    if not static_edges_df.empty:\n",
        "        for _, edge in static_edges_df.iterrows():\n",
        "            src_nodes = iso_to_indices.get(edge['source'], [])\n",
        "            tgt_nodes = iso_to_indices.get(edge['target'], [])\n",
        "            for i in src_nodes:\n",
        "                for j in tgt_nodes:\n",
        "                    adjacency[i, j] = 1.0\n",
        "                    adjacency[j, i] = 1.0\n",
        "    if not dyn_edges_df.empty and snapshot_time is not None:\n",
        "        window_start = snapshot_time - pd.Timedelta(hours=lookback_hours)\n",
        "        mask = dyn_edges_df['timestamp'].between(window_start, snapshot_time)\n",
        "        for _, edge in dyn_edges_df.loc[mask].iterrows():\n",
        "            src_nodes = iso_to_indices.get(edge['source_iso3'], [])\n",
        "            tgt_nodes = iso_to_indices.get(edge['target_iso3'], [])\n",
        "            for i in src_nodes:\n",
        "                for j in tgt_nodes:\n",
        "                    adjacency[i, j] += 1.0\n",
        "                    adjacency[j, i] += 1.0\n",
        "    if n > 0:\n",
        "        np.fill_diagonal(adjacency, 1.0)\n",
        "    return _normalize_adjacency(adjacency)\n",
        "\n",
        "def compute_market_features(nodes_df: pd.DataFrame, indicator_cols: list) -> tuple:\n",
        "    values = []\n",
        "    names = []\n",
        "    for col in indicator_cols:\n",
        "        if col not in nodes_df.columns:\n",
        "            continue\n",
        "        col_values = pd.to_numeric(nodes_df[col], errors='coerce')\n",
        "        values.extend([\n",
        "            np.nanmean(col_values.values),\n",
        "            np.nanstd(col_values.values)\n",
        "        ])\n",
        "        names.extend([f\"{col}_mean\", f\"{col}_std\"])\n",
        "    if 'wti_price' in nodes_df.columns:\n",
        "        latest_price = pd.to_numeric(nodes_df['wti_price'], errors='coerce').iloc[-1]\n",
        "        values.append(latest_price)\n",
        "        names.append('wti_price_current')\n",
        "    if 'brent_price' in nodes_df.columns:\n",
        "        latest_brent = pd.to_numeric(nodes_df['brent_price'], errors='coerce').iloc[-1]\n",
        "        values.append(latest_brent)\n",
        "        names.append('brent_price_current')\n",
        "    values = np.nan_to_num(np.array(values, dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return values, names\n",
        "\n",
        "def extract_engineered_features(nodes_df: pd.DataFrame, engineered_cols: list) -> tuple:\n",
        "    values = []\n",
        "    names = []\n",
        "    for col in engineered_cols:\n",
        "        if col not in nodes_df.columns:\n",
        "            continue\n",
        "        col_values = pd.to_numeric(nodes_df[col], errors='coerce')\n",
        "        if col_values.notna().any():\n",
        "            values.append(float(col_values.dropna().iloc[0]))\n",
        "        else:\n",
        "            values.append(0.0)\n",
        "        names.append(col)\n",
        "    if not values:\n",
        "        return np.array([], dtype=float), []\n",
        "    values = np.nan_to_num(np.array(values, dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return values, names\n",
        "\n",
        "def build_engineered_feature_vector(nodes_df: pd.DataFrame) -> tuple:\n",
        "    market_vector, market_names = compute_market_features(nodes_df, market_indicator_cols)\n",
        "    engineered_vector, engineered_names = extract_engineered_features(nodes_df, engineered_feature_cols)\n",
        "    feature_blocks = [block for block in [market_vector, engineered_vector] if block.size > 0]\n",
        "    if not feature_blocks:\n",
        "        return np.array([], dtype=float), []\n",
        "    combined = np.concatenate(feature_blocks)\n",
        "    feature_names = market_names + engineered_names\n",
        "    return combined, feature_names\n",
        "\n",
        "def build_snapshots(node_features: pd.DataFrame, static_edges_df: pd.DataFrame, dyn_edges_df: pd.DataFrame, price_by_date: pd.DataFrame, lookback_hours: int = 24) -> tuple:\n",
        "    engineered_rows = []\n",
        "    targets = []\n",
        "    dates = []\n",
        "    snapshots = []\n",
        "    feature_names = None\n",
        "    for ts in sorted(node_features['date'].unique()):\n",
        "        ts = pd.Timestamp(ts)\n",
        "        nodes_df = node_features[node_features['date'] == ts].copy()\n",
        "        if nodes_df.empty:\n",
        "            continue\n",
        "        adjacency = build_adjacency_matrix(nodes_df, static_edges_df, dyn_edges_df, ts, lookback_hours=lookback_hours)\n",
        "        node_matrix = nodes_df[feature_columns_graph].apply(pd.to_numeric, errors='coerce').fillna(0.0).to_numpy(dtype=float)\n",
        "        if node_matrix.size == 0:\n",
        "            continue\n",
        "        engineered_vector, names = build_engineered_feature_vector(nodes_df)\n",
        "        if engineered_vector.size == 0:\n",
        "            continue\n",
        "        target_series = price_by_date['wti_ret_next'].reindex([ts]) if 'wti_ret_next' in price_by_date.columns else price_by_date['wti_delta_next'].reindex([ts])\n",
        "        target = target_series.iloc[0] if ts in target_series.index else np.nan\n",
        "        if pd.isna(target):\n",
        "            continue\n",
        "        if feature_names is None:\n",
        "            feature_names = names\n",
        "        engineered_rows.append(engineered_vector.astype(np.float32))\n",
        "        targets.append(float(target))\n",
        "        dates.append(ts)\n",
        "        snapshots.append(\n",
        "            GraphSnapshot(\n",
        "                date=ts,\n",
        "                node_features=node_matrix.astype(np.float32),\n",
        "                adjacency=adjacency.astype(np.float32),\n",
        "                engineered_features=engineered_vector.astype(np.float32),\n",
        "                target=float(target)\n",
        "            )\n",
        "        )\n",
        "    if not engineered_rows:\n",
        "        raise ValueError('No graph snapshots were built; verify that node features and price targets overlap.')\n",
        "    engineered_matrix = np.vstack(engineered_rows)\n",
        "    targets = np.array(targets, dtype=np.float32)\n",
        "    feature_names = feature_names or []\n",
        "    return engineered_matrix, targets, dates, feature_names, snapshots"
      ],
      "metadata": {
        "id": "0qTQUH9qri0W"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
        "  y_true = np.asarray(y_true, dtype=float)\n",
        "  y_pred = np.asarray(y_pred, dtype=float)\n",
        "  if y_true.size == 0 or y_pred.size == 0:\n",
        "      return {\n",
        "          'mae': float('nan'),\n",
        "          'rmse': float('nan'),\n",
        "          'r2': float('nan'),\n",
        "          'directional_accuracy': float('nan')\n",
        "      }\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  rmse = mean_squared_error(y_true, y_pred)\n",
        "  r2 = r2_score(y_true, y_pred) if y_true.size > 1 else float('nan')\n",
        "  mask = (y_true != 0) | (y_pred != 0)\n",
        "  if mask.any():\n",
        "      da = float((np.sign(y_true[mask]) == np.sign(y_pred[mask])).mean())\n",
        "  else:\n",
        "      da = float('nan')\n",
        "  return {\n",
        "      'mae': float(mae),\n",
        "      'rmse': float(rmse),\n",
        "      'r2': float(r2),\n",
        "      'directional_accuracy': da\n",
        "  }\n",
        "\n",
        "\n",
        "def train_tabular_baseline(engineered_matrix: np.ndarray, targets: np.ndarray, dates: list, feature_names: list, split_index: int) -> dict:\n",
        "  if engineered_matrix.ndim == 1:\n",
        "      engineered_matrix = engineered_matrix.reshape(-1, 1)\n",
        "  X_train = engineered_matrix[:split_index]\n",
        "  X_test = engineered_matrix[split_index:]\n",
        "  y_train = targets[:split_index]\n",
        "  y_test = targets[split_index:]\n",
        "  train_dates = dates[:split_index]\n",
        "  test_dates = dates[split_index:]\n",
        "\n",
        "  model = lgb.LGBMRegressor(\n",
        "      learning_rate=0.05,\n",
        "      n_estimators=400,\n",
        "      num_leaves=31,\n",
        "      feature_fraction=0.9,\n",
        "      bagging_fraction=0.8,\n",
        "      bagging_freq=5,\n",
        "      min_data_in_leaf=10,\n",
        "      random_state=42\n",
        "  )\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  train_pred = model.predict(X_train)\n",
        "  test_pred = model.predict(X_test) if len(X_test) else np.array([], dtype=float)\n",
        "\n",
        "  metrics = {\n",
        "      'train': compute_regression_metrics(y_train, train_pred),\n",
        "      'test': compute_regression_metrics(y_test, test_pred)\n",
        "  }\n",
        "\n",
        "  feature_importance = pd.DataFrame()\n",
        "  if feature_names:\n",
        "      booster = model.booster_\n",
        "      feature_importance = pd.DataFrame({\n",
        "          'feature': feature_names,\n",
        "          'importance_gain': booster.feature_importance(importance_type='gain'),\n",
        "          'importance_split': booster.feature_importance(importance_type='split')\n",
        "      }).sort_values('importance_gain', ascending=False).reset_index(drop=True)\n",
        "\n",
        "  predictions = {\n",
        "      'train': pd.DataFrame({'date': train_dates, 'target': y_train, 'prediction': train_pred}),\n",
        "      'test': pd.DataFrame({'date': test_dates, 'target': y_test, 'prediction': test_pred})\n",
        "  }"
      ],
      "metadata": {
        "id": "oyfkhgXMwTVs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 64, embedding_dim: int = 32):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lin1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lin2 = nn.Linear(hidden_dim, embedding_dim)\n",
        "        self.readout = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, features: torch.Tensor, adjacency: torch.Tensor) -> tuple:\n",
        "        h = adjacency @ features\n",
        "        h = torch.relu(self.lin1(h))\n",
        "        h = adjacency @ h\n",
        "        h = torch.relu(self.lin2(h))\n",
        "        graph_embedding = h.mean(dim=0)\n",
        "        output = self.readout(graph_embedding)\n",
        "        return output.squeeze(-1), graph_embedding\n",
        "\n",
        "def _predict_with_gnn(model: SimpleGCN, snapshots: list) -> tuple:\n",
        "    predictions = []\n",
        "    embeddings = []\n",
        "    dates = []\n",
        "    for snap in snapshots:\n",
        "        features = torch.from_numpy(snap.node_features).float()\n",
        "        adjacency = torch.from_numpy(snap.adjacency).float()\n",
        "        with torch.no_grad():\n",
        "            pred, embedding = model(features, adjacency)\n",
        "        predictions.append(pred.item())\n",
        "        embeddings.append(embedding.detach().cpu().numpy())\n",
        "        dates.append(snap.date)\n",
        "    if embeddings:\n",
        "        embedding_matrix = np.vstack(embeddings).astype(np.float32)\n",
        "    else:\n",
        "        embedding_matrix = np.empty((0, getattr(model, 'embedding_dim', 0)), dtype=np.float32)\n",
        "    predictions = np.array(predictions, dtype=np.float32)\n",
        "    return predictions, embedding_matrix, dates\n",
        "\n",
        "def train_gnn_model(snapshots: list, split_index: int, epochs: int = 200, learning_rate: float = 1e-3, hidden_dim: int = 64, embedding_dim: int = 32, weight_decay: float = 1e-4) -> dict:\n",
        "    if not snapshots:\n",
        "        raise ValueError('No graph snapshots provided for GNN training.')\n",
        "\n",
        "    input_dim = snapshots[0].node_features.shape[1]\n",
        "    model = SimpleGCN(input_dim, hidden_dim=hidden_dim, embedding_dim=embedding_dim)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    train_snapshots = snapshots[:split_index]\n",
        "    test_snapshots = snapshots[split_index:]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for snap in train_snapshots:\n",
        "            features = torch.from_numpy(snap.node_features).float()\n",
        "            adjacency = torch.from_numpy(snap.adjacency).float()\n",
        "            target = torch.tensor(snap.target, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            prediction, _ = model(features, adjacency)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        if (epoch + 1) % max(1, epochs // 5) == 0:\n",
        "            avg_loss = total_loss / max(1, len(train_snapshots))\n",
        "            print(f\"Epoch {epoch + 1}/{epochs} - Train loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    train_pred, train_embeddings, train_dates = _predict_with_gnn(model, train_snapshots)\n",
        "    test_pred, test_embeddings, test_dates = _predict_with_gnn(model, test_snapshots)\n",
        "    all_pred, all_embeddings, all_dates = _predict_with_gnn(model, snapshots)\n",
        "\n",
        "    train_targets = np.array([snap.target for snap in train_snapshots], dtype=np.float32)\n",
        "    test_targets = np.array([snap.target for snap in test_snapshots], dtype=np.float32)\n",
        "\n",
        "    metrics = {\n",
        "        'train': compute_regression_metrics(train_targets, train_pred),\n",
        "        'test': compute_regression_metrics(test_targets, test_pred)\n",
        "    }\n",
        "\n",
        "    predictions = {\n",
        "        'train': pd.DataFrame({'date': train_dates, 'target': train_targets, 'prediction': train_pred}),\n",
        "        'test': pd.DataFrame({'date': test_dates, 'target': test_targets, 'prediction': test_pred})\n",
        "    }\n",
        "\n",
        "    embedding_names = [f'gnn_embedding_{i}' for i in range(all_embeddings.shape[1])] if all_embeddings.size else []\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'metrics': metrics,\n",
        "        'predictions': predictions,\n",
        "        'embeddings': {\n",
        "            'train': train_embeddings,\n",
        "            'test': test_embeddings,\n",
        "            'all': all_embeddings,\n",
        "            'dates': all_dates,\n",
        "            'feature_names': embedding_names\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "Kw4oOqFYz23b"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "def train_hybrid_lightgbm(engineered_matrix: np.ndarray, gnn_embeddings: np.ndarray, targets: np.ndarray, dates: list, engineered_feature_names: list, gnn_feature_names: list, split_index: int) -> dict:\n",
        "    if engineered_matrix.ndim == 1:\n",
        "        engineered_matrix = engineered_matrix.reshape(-1, 1)\n",
        "    if gnn_embeddings.ndim == 1:\n",
        "        gnn_embeddings = gnn_embeddings.reshape(-1, 1)\n",
        "    if engineered_matrix.shape[0] != gnn_embeddings.shape[0]:\n",
        "        raise ValueError('Engineered features and GNN embeddings must align on samples.')\n",
        "\n",
        "    combined_features = np.hstack([engineered_matrix, gnn_embeddings])\n",
        "    feature_names = list(engineered_feature_names) + list(gnn_feature_names)\n",
        "    if not feature_names:\n",
        "        feature_names = [f'feature_{i}' for i in range(combined_features.shape[1])]\n",
        "\n",
        "    X_train = combined_features[:split_index]\n",
        "    X_test = combined_features[split_index:]\n",
        "    y_train = targets[:split_index]\n",
        "    y_test = targets[split_index:]\n",
        "    train_dates = dates[:split_index]\n",
        "    test_dates = dates[split_index:]\n",
        "\n",
        "    X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
        "    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "\n",
        "    validation_fraction = 0.2\n",
        "    min_train_points = 5\n",
        "    if len(X_train_df) >= min_train_points:\n",
        "        val_size = max(1, int(len(X_train_df) * validation_fraction))\n",
        "        if val_size >= len(X_train_df):\n",
        "            val_size = len(X_train_df) - 1\n",
        "        val_start = len(X_train_df) - val_size\n",
        "        X_fit_df = X_train_df.iloc[:val_start]\n",
        "        y_fit = y_train[:val_start]\n",
        "        X_val_df = X_train_df.iloc[val_start:]\n",
        "        y_val = y_train[val_start:]\n",
        "        eval_set = [(X_val_df, y_val)]\n",
        "        callbacks = [lgb.early_stopping(50, verbose=False), lgb.log_evaluation(period=0)]\n",
        "    else:\n",
        "        X_fit_df = X_train_df\n",
        "        y_fit = y_train\n",
        "        eval_set = None\n",
        "        callbacks = None\n",
        "\n",
        "    model = lgb.LGBMRegressor(\n",
        "        learning_rate=0.05,\n",
        "        n_estimators=500,\n",
        "        num_leaves=31,\n",
        "        subsample=0.8,\n",
        "        subsample_freq=5,\n",
        "        colsample_bytree=0.9,\n",
        "        min_data_in_leaf=10,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    fit_kwargs = {'feature_name': feature_names}\n",
        "    if eval_set:\n",
        "        fit_kwargs.update({\n",
        "            'eval_set': eval_set,\n",
        "            'eval_names': ['validation'],\n",
        "            'eval_metric': 'l2',\n",
        "            'callbacks': callbacks\n",
        "        })\n",
        "\n",
        "    model.fit(X_fit_df, y_fit, **fit_kwargs)\n",
        "\n",
        "    predict_kwargs = {}\n",
        "    if getattr(model, 'best_iteration_', None) is not None:\n",
        "        predict_kwargs['num_iteration'] = model.best_iteration_\n",
        "\n",
        "    train_pred = model.predict(X_train_df, **predict_kwargs)\n",
        "    test_pred = model.predict(X_test_df, **predict_kwargs) if len(X_test_df) else np.array([], dtype=float)\n",
        "\n",
        "    metrics = {\n",
        "        'train': compute_regression_metrics(y_train, train_pred),\n",
        "        'test': compute_regression_metrics(y_test, test_pred)\n",
        "    }\n",
        "\n",
        "    booster = model.booster_\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance_gain': booster.feature_importance(importance_type='gain'),\n",
        "        'importance_split': booster.feature_importance(importance_type='split')\n",
        "    }).sort_values('importance_gain', ascending=False).reset_index(drop=True)\n",
        "\n",
        "    predictions = {\n",
        "        'train': pd.DataFrame({'date': train_dates, 'target': y_train, 'prediction': train_pred}),\n",
        "        'test': pd.DataFrame({'date': test_dates, 'target': y_test, 'prediction': test_pred})\n",
        "    }\n",
        "\n",
        "    data_splits = {\n",
        "        'X_train': X_train_df,\n",
        "        'X_test': X_test_df,\n",
        "        'y_train': y_train,\n",
        "        'y_test': y_test,\n",
        "        'train_dates': train_dates,\n",
        "        'test_dates': test_dates,\n",
        "        'feature_names': feature_names,\n",
        "        'best_iteration': getattr(model, 'best_iteration_', None)\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'feature_names': feature_names,\n",
        "        'metrics': metrics,\n",
        "        'feature_importance': feature_importance,\n",
        "        'predictions': predictions,\n",
        "        'data': data_splits\n",
        "    }\n",
        "\n",
        "def run_modeling_pipeline(node_features: pd.DataFrame, static_edges_df: pd.DataFrame, dyn_edges_df: pd.DataFrame, price_by_date: pd.DataFrame, lookback_hours: int = 24, test_fraction: float = 0.2, gnn_params: Optional[dict] = None) -> dict:\n",
        "    engineered_matrix, targets, dates, engineered_feature_names, snapshots = build_snapshots(\n",
        "        node_features, static_edges_df, dyn_edges_df, price_by_date, lookback_hours=lookback_hours\n",
        "    )\n",
        "    n_samples = len(targets)\n",
        "    if n_samples == 0:\n",
        "        raise ValueError('No samples available for training pipeline.')\n",
        "\n",
        "    split_index = int(n_samples * (1 - test_fraction))\n",
        "    split_index = max(1, min(split_index, n_samples))\n",
        "\n",
        "\n",
        "    gnn_kwargs = gnn_params or {}\n",
        "    gnn_results = train_gnn_model(snapshots, split_index, **gnn_kwargs)\n",
        "    gnn_embeddings = gnn_results['embeddings']['all']\n",
        "    gnn_feature_names = gnn_results['embeddings']['feature_names']\n",
        "\n",
        "    hybrid_results = train_hybrid_lightgbm(\n",
        "        engineered_matrix,\n",
        "        gnn_embeddings,\n",
        "        targets,\n",
        "        dates,\n",
        "        engineered_feature_names,\n",
        "        gnn_feature_names,\n",
        "        split_index\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'engineered_matrix': engineered_matrix,\n",
        "        'targets': targets,\n",
        "        'dates': dates,\n",
        "        'snapshots': snapshots,\n",
        "        'split_index': split_index,\n",
        "        'gnn': gnn_results,\n",
        "        'hybrid': hybrid_results\n",
        "    }"
      ],
      "metadata": {
        "id": "bbazdOda05je"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_results = run_modeling_pipeline(\n",
        "    node_features=node_features,\n",
        "    static_edges_df=static_edges_df,\n",
        "    dyn_edges_df=dyn_edges_df,\n",
        "    price_by_date=price_by_date,\n",
        "    lookback_hours=24,\n",
        "    test_fraction=0.2,\n",
        "    gnn_params={'epochs': 150, 'hidden_dim': 64, 'embedding_dim': 32, 'learning_rate': 1e-3}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbyCj_7m467N",
        "outputId": "b535feba-ff4a-4910-c315-430477725c1b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/150 - Train loss: 0.0004\n",
            "Epoch 60/150 - Train loss: 0.0004\n",
            "Epoch 90/150 - Train loss: 0.0004\n",
            "Epoch 120/150 - Train loss: 0.0004\n",
            "Epoch 150/150 - Train loss: 0.0004\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000490 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 2293\n",
            "[LightGBM] [Info] Number of data points in the train set: 468, number of used features: 17\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Info] Start training from score -0.000008\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_records = []\n",
        "missing_models = []\n",
        "for model_name in ['hybrid']:\n",
        "    stage_result = pipeline_results.get(model_name)\n",
        "    if not stage_result:\n",
        "        missing_models.append(model_name)\n",
        "        continue\n",
        "    metrics = stage_result.get('metrics')\n",
        "    if not isinstance(metrics, dict):\n",
        "        raise ValueError(f\"Metrics for '{model_name}' stage are unavailable.\")\n",
        "    for split_name, metric_values in metrics.items():\n",
        "        if not isinstance(metric_values, dict):\n",
        "            continue\n",
        "        record = {'model': model_name, 'split': split_name}\n",
        "        record.update(metric_values)\n",
        "        metrics_records.append(record)\n",
        "\n",
        "if missing_models:\n",
        "    raise ValueError(\n",
        "        'No results were produced for the following stages: ' + ', '.join(missing_models) + '. Check earlier logs for training errors.'\n",
        "    )\n",
        "\n",
        "metrics_table = pd.DataFrame(metrics_records)\n",
        "metrics_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "TWWtfvFeA4TU",
        "outputId": "eb2d5b04-9882-4ad6-fe14-66af74cbdd75"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    model  split       mae      rmse        r2  directional_accuracy\n",
              "0  hybrid  train  0.014741  0.000353  0.182487              0.748718\n",
              "1  hybrid   test  0.016244  0.000442 -0.002574              0.537415"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de594d52-cb18-4979-8ee6-61f278b6e0b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>split</th>\n",
              "      <th>mae</th>\n",
              "      <th>rmse</th>\n",
              "      <th>r2</th>\n",
              "      <th>directional_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hybrid</td>\n",
              "      <td>train</td>\n",
              "      <td>0.014741</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.182487</td>\n",
              "      <td>0.748718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hybrid</td>\n",
              "      <td>test</td>\n",
              "      <td>0.016244</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>-0.002574</td>\n",
              "      <td>0.537415</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de594d52-cb18-4979-8ee6-61f278b6e0b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-de594d52-cb18-4979-8ee6-61f278b6e0b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-de594d52-cb18-4979-8ee6-61f278b6e0b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1121ed6a-ed50-4d3e-908f-578066bf346c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1121ed6a-ed50-4d3e-908f-578066bf346c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1121ed6a-ed50-4d3e-908f-578066bf346c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_85e55c97-b748-4864-be06-9a9a894b47e9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_table')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_85e55c97-b748-4864-be06-9a9a894b47e9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_table');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_table",
              "summary": "{\n  \"name\": \"metrics_table\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"hybrid\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"test\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0010621610647817085,\n        \"min\": 0.014741435028034317,\n        \"max\": 0.016243557611273256,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.016243557611273256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.263397189383416e-05,\n        \"min\": 0.0003531730710110568,\n        \"max\": 0.00044175088352861234,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.00044175088352861234\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"r2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13085803579156077,\n        \"min\": -0.0025737998035719745,\n        \"max\": 0.18248740915835715,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.0025737998035719745\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"directional_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14941377197442587,\n        \"min\": 0.5374149659863946,\n        \"max\": 0.7487179487179487,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5374149659863946\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0j1TMNmhjNC",
        "outputId": "08fee126-67a9-45ca-80eb-a615fdc7a8e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model and evaluation data to GCS...\n",
            "Staging files in /tmp/model_export...\n",
            "Uploading artifacts to GCS bucket 'gdelt_raw_3_years' at prefix 'processed_data/model_artifacts/'...\n",
            "\n",
            "--- Upload Complete ---\n",
            "Artifacts saved to GCS:\n",
            "  - Model: gs://gdelt_raw_3_years/processed_data/model_artifacts/gbs_model.joblib\n",
            "  - X_test: gs://gdelt_raw_3_years/processed_data/model_artifacts/X_test.npy (shape: (147, 24))\n",
            "  - y_test: gs://gdelt_raw_3_years/processed_data/model_artifacts/y_test.npy (shape: (147,))\n",
            "  - Features: gs://gdelt_raw_3_years/processed_data/model_artifacts/feature_names.json (count: 24)\n",
            "\n",
            "Cleaned up temporary directory: /tmp/model_export\n"
          ]
        }
      ],
      "source": [
        "# --- CELL TO SAVE MODEL ARTIFACTS TO GCS ---\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "\n",
        "print(\"Saving model and evaluation data to GCS...\")\n",
        "\n",
        "# --- 1. Define Paths ---\n",
        "# Use a temporary local directory for staging files\n",
        "local_tmp_dir = \"/tmp/model_export\"\n",
        "os.makedirs(local_tmp_dir, exist_ok=True)\n",
        "\n",
        "# Define the GCS prefix where artifacts will be stored\n",
        "gcs_model_prefix = PROCESSED_PREFIX + \"model_artifacts/\"\n",
        "\n",
        "# Define local paths\n",
        "local_model_path = os.path.join(local_tmp_dir, \"gbs_model.joblib\")\n",
        "local_x_test_path = os.path.join(local_tmp_dir, \"X_test.npy\")\n",
        "local_y_test_path = os.path.join(local_tmp_dir, \"y_test.npy\")\n",
        "local_features_path = os.path.join(local_tmp_dir, \"feature_names.json\")\n",
        "\n",
        "# --- 2. Retrieve Data to Save ---\n",
        "model_to_save = pipeline_results['model']\n",
        "feature_names_to_save = pipeline_results['feature_names']\n",
        "\n",
        "# Re-build snapshots to get the correct test set split\n",
        "X_all, y_all, dates_all, _, _ = build_snapshots(\n",
        "    node_features, static_edges_df, dyn_edges_df, price_by_date, lookback_hours=24\n",
        ")\n",
        "n_samples = len(X_all)\n",
        "split_index = max(1, int(n_samples * (1 - 0.2)))\n",
        "X_test = X_all[split_index:]\n",
        "y_test = y_all[split_index:]\n",
        "\n",
        "\n",
        "# --- 3. Save Files Locally ---\n",
        "print(f\"Staging files in {local_tmp_dir}...\")\n",
        "# 1. Save the model\n",
        "joblib.dump(model_to_save, local_model_path)\n",
        "\n",
        "# 2. Save the test data\n",
        "np.save(local_x_test_path, X_test)\n",
        "np.save(local_y_test_path, y_test)\n",
        "\n",
        "# 3. Save the feature names\n",
        "with open(local_features_path, 'w') as f:\n",
        "    json.dump(feature_names_to_save, f)\n",
        "\n",
        "\n",
        "# --- 4. Upload Files to GCS ---\n",
        "print(f\"Uploading artifacts to GCS bucket '{BUCKET_NAME}' at prefix '{gcs_model_prefix}'...\")\n",
        "\n",
        "files_to_upload = [\n",
        "    (\"gbs_model.joblib\", local_model_path),\n",
        "    (\"X_test.npy\", local_x_test_path),\n",
        "    (\"y_test.npy\", local_y_test_path),\n",
        "    (\"feature_names.json\", local_features_path)\n",
        "]\n",
        "\n",
        "gcs_paths = {}\n",
        "\n",
        "for filename, local_path in files_to_upload:\n",
        "    try:\n",
        "        gcs_path = f\"{gcs_model_prefix}{filename}\"\n",
        "        blob = bucket.blob(gcs_path)\n",
        "        blob.upload_from_filename(local_path)\n",
        "        gcs_paths[filename] = f\"gs://{BUCKET_NAME}/{gcs_path}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR uploading {filename}: {e}\")\n",
        "\n",
        "print(\"\\n--- Upload Complete ---\")\n",
        "print(f\"Artifacts saved to GCS:\")\n",
        "print(f\"  - Model: {gcs_paths.get('gbs_model.joblib')}\")\n",
        "print(f\"  - X_test: {gcs_paths.get('X_test.npy')} (shape: {X_test.shape})\")\n",
        "print(f\"  - y_test: {gcs_paths.get('y_test.npy')} (shape: {y_test.shape})\")\n",
        "print(f\"  - Features: {gcs_paths.get('feature_names.json')} (count: {len(feature_names_to_save)})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/whssxnd3ys31xXXpqxLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}