{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKmgnPVnXkwk3YziZ596ob",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishee82oo/nfs-oil-price-prediction/blob/main/GDELT_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gXr_gIiBYl3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0524a941-d612-4b54-8a7d-e877a5a9fe3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage pyarrow pycountry tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util, sys, os, io, json, gzip\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd, numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "C8_bX21FYp5P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except Exception:\n",
        "    pass\n",
        "import google.auth\n",
        "from google.cloud import storage\n",
        "credentials, default_project = google.auth.default()\n",
        "client = storage.Client(project=default_project, credentials=credentials)"
      ],
      "metadata": {
        "id": "48mIZtkIYrIa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data from GCS"
      ],
      "metadata": {
        "id": "2uPZ2jLAZIk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spec = importlib.util.spec_from_file_location(\"gdelt_module\", \"/mnt/data/gdelt_data_engineering_clean.ipynb\")\n",
        "gdelt_module = None\n",
        "if spec and spec.loader:\n",
        "    gm = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(gm)\n",
        "    gdelt_module = gm\n",
        "if gdelt_module and hasattr(gdelt_module, \"GCS_BUCKET_NAME\"):\n",
        "    BUCKET_NAME = getattr(gdelt_module, \"GCS_BUCKET_NAME\")\n",
        "else:\n",
        "    BUCKET_NAME = os.environ.get(\"GCS_BUCKET_NAME\", \"gdelt_raw_3_years\")\n",
        "if gdelt_module and hasattr(gdelt_module, \"GCS_PROCESSED_PATH\"):\n",
        "    PROCESSED_PREFIX = getattr(gdelt_module, \"GCS_PROCESSED_PATH\")\n",
        "else:\n",
        "    PROCESSED_PREFIX = os.environ.get(\"GCS_PROCESSED_PATH\", \"processed_data/\")\n",
        "bucket = client.bucket(BUCKET_NAME)\n",
        "blobs = list(client.list_blobs(BUCKET_NAME, prefix=PROCESSED_PREFIX))\n",
        "final_blobs = [b for b in blobs if b.name.startswith(f\"{PROCESSED_PREFIX}final_aligned_data_\") and b.name.endswith(\".json.gz\")]\n",
        "final_blobs_sorted = sorted(final_blobs, key=lambda b: b.name, reverse=True)\n",
        "if len(final_blobs_sorted)==0:\n",
        "    raise SystemExit(\"No final_aligned_data_*.json.gz found under the processed prefix\")\n",
        "latest_blob = final_blobs_sorted[0]\n",
        "local_download_path = \"/tmp/latest_final_aligned_data.json.gz\"\n",
        "with open(local_download_path, \"wb\") as f:\n",
        "    f.write(latest_blob.download_as_bytes())"
      ],
      "metadata": {
        "id": "XaZWM8XTYs7j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forming Graph"
      ],
      "metadata": {
        "id": "wyfaKsH_ZPWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(local_download_path):\n",
        "    file_size = os.path.getsize(local_download_path) / (1024*1024)\n",
        "    print(f\"File downloaded: {local_download_path}\")\n",
        "    print(f\"Size: {file_size:.2f} MB\")\n",
        "\n",
        "else:\n",
        "    print(\"File NOT found locally\")\n",
        "\n",
        "print(f\"Downloaded from GCS: {latest_blob.name}\")\n",
        "print(f\"Bucket: {BUCKET_NAME}\")\n",
        "print(f\"Last modified: {latest_blob.updated}\")\n",
        "\n",
        "with gzip.open(local_download_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "    sample = f.read(500)\n",
        "    print(f\"Data preview:\\n{sample[:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4tTv_JeHPeD",
        "outputId": "1b557db1-1d57-47ca-9b0f-84f34e79b383"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded: /tmp/latest_final_aligned_data.json.gz\n",
            "Size: 2.37 MB\n",
            "Downloaded from GCS: processed_data/final_aligned_data_20250908.json.gz\n",
            "Bucket: gdelt_raw_3_years\n",
            "Last modified: 2025-09-08 01:30:57.993000+00:00\n",
            "Data preview:\n",
            "[{\"date\": \"20220825\", \"country\": \"US\", \"event_count\": 2301, \"avg_sentiment\": -0.023938528465884453, \"unique_sources\": 728, \"wti_price\": 93.33, \"brent_price\": 98.81, \"theme_energy\": 45, \"theme_conflict...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open(local_download_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "    raw = f.read()\n",
        "records = json.loads(raw)\n",
        "df = pd.DataFrame.from_records(records)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "df = df.sort_values(\"date\").reset_index(drop=True)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "AseuuqqNYuqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb993b45-16dc-4d09-b838-d2055595c028"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143401, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pycountry\n",
        "def to_iso3(name):\n",
        "    try:\n",
        "        c = pycountry.countries.lookup(name)\n",
        "        return c.alpha_3\n",
        "    except Exception:\n",
        "        s = str(name).upper()\n",
        "        s2 = \"\".join([c for c in s if c.isalpha() or c==\" \"]).strip().replace(\" \", \"_\")\n",
        "        return s2\n",
        "df[\"country_iso3\"] = df[\"country\"].fillna(\"UNKNOWN\").apply(to_iso3)\n",
        "df[\"node_id\"] = df[\"country_iso3\"].astype(str) + \"_\" + df[\"date\"].dt.strftime(\"%Y%m%d\")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "DYBXmOXTYx7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e163bc4-583f-4c2e-dff9-0bc99872532b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143401, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "price_by_date = df[[\"date\",\"wti_price\"]].drop_duplicates().set_index(\"date\").sort_index()\n",
        "price_by_date[\"wti_price\"] = pd.to_numeric(price_by_date[\"wti_price\"], errors=\"coerce\")\n",
        "price_by_date = price_by_date.sort_index()\n",
        "price_by_date[\"wti_price_next\"] = price_by_date[\"wti_price\"].shift(-1)\n",
        "price_by_date[\"wti_delta_next\"] = price_by_date[\"wti_price_next\"] - price_by_date[\"wti_price\"]\n",
        "price_by_date[\"wti_ret_next\"] = price_by_date[\"wti_delta_next\"] / price_by_date[\"wti_price\"]\n",
        "price_map = price_by_date.to_dict(orient=\"index\")\n",
        "def attach_targets(row):\n",
        "    pdx = row[\"date\"]\n",
        "    v = price_map.get(pdx)\n",
        "    if v is None:\n",
        "        return pd.Series([np.nan,np.nan])\n",
        "    return pd.Series([v.get(\"wti_delta_next\"), v.get(\"wti_ret_next\")])\n",
        "df[[\"wti_delta_next\",\"wti_ret_next\"]] = df.apply(attach_targets, axis=1)"
      ],
      "metadata": {
        "id": "qFVQHJNdYyka"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [c for c in df.columns if c not in [\"country\",\"date\",\"node_id\",\"country_iso3\",\"wti_delta_next\",\"wti_ret_next\"]]\n",
        "feature_cols = [c for c in feature_cols if df[c].dtype != \"object\" or c.startswith(\"theme_\")]\n",
        "node_features = df[[\"node_id\",\"country\",\"country_iso3\",\"date\",\"wti_price\",\"brent_price\",\"wti_delta_next\",\"wti_ret_next\"] + [c for c in df.columns if c in feature_cols]]\n",
        "node_features = node_features.fillna(0)"
      ],
      "metadata": {
        "id": "cZL1fJFZY0Li"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opec_members = [\"Venezuela\",\"Saudi Arabia\",\"Iran\",\"Iraq\",\"Kuwait\",\"UAE\",\"Qatar\",\"Algeria\",\"Angola\",\"Libya\",\"Nigeria\",\"Ecuador\",\"Gabon\",\"Republic of the Congo\",\"Equatorial Guinea\"]\n",
        "iso_map = {}\n",
        "for name in opec_members:\n",
        "    try:\n",
        "        iso_map[name] = pycountry.countries.lookup(name).alpha_3\n",
        "    except Exception:\n",
        "        iso_map[name] = name.upper().replace(\" \",\"_\")\n",
        "opec_iso = list(iso_map.values())\n",
        "from itertools import combinations\n",
        "static_edges = []\n",
        "for a,b in combinations(opec_iso,2):\n",
        "    static_edges.append({\"source\":a,\"target\":b,\"edge_type\":\"opec_member\"})\n",
        "static_edges_df = pd.DataFrame(static_edges)"
      ],
      "metadata": {
        "id": "mN4YlVMFY2Hv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dyn_edges_df = pd.DataFrame(columns=[\"source\",\"target\",\"edge_type\",\"timestamp\",\"date\"])\n",
        "potential_actor_cols = [c for c in df.columns if \"actor\" in c.lower() or \"actor1\" in c.lower() or \"actor2\" in c.lower()]\n",
        "if len(potential_actor_cols)>0:\n",
        "    actors = []\n",
        "    for idx,row in df.iterrows():\n",
        "        for k in potential_actor_cols:\n",
        "            v = row.get(k)\n",
        "        if row.get(\"actor1\") and row.get(\"actor2\"):\n",
        "            s = str(row.get(\"actor1\"))\n",
        "            t = str(row.get(\"actor2\"))\n",
        "            dyn_edges_df.loc[len(dyn_edges_df)] = [s,t,\"gdelt_event\",row.get(\"date\"),row.get(\"date\")]\n"
      ],
      "metadata": {
        "id": "ccoFJtmnY4gH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "feature_cols = [c for c in df.columns if c not in [\"country\",\"date\",\"node_id\",\"country_iso3\",\"wti_delta_next\",\"wti_ret_next\",\"wti_price\",\"brent_price\"]]\n",
        "feature_cols = [c for c in feature_cols if df[c].dtype != \"object\" or c.startswith(\"theme_\")]\n",
        "node_features = df[[\"node_id\",\"country\",\"country_iso3\",\"date\",\"wti_price\",\"brent_price\",\"wti_delta_next\",\"wti_ret_next\"] + [c for c in df.columns if c in feature_cols]]\n",
        "node_features = node_features.loc[:, ~node_features.columns.duplicated()]\n",
        "node_features = node_features.fillna(0)\n",
        "\n",
        "out_dir_local = \"/tmp/graph_export\"\n",
        "os.makedirs(out_dir_local, exist_ok=True)\n",
        "nodes_out = os.path.join(out_dir_local, \"nodes.parquet\")\n",
        "static_edges_out = os.path.join(out_dir_local, \"edges_static.parquet\")\n",
        "dyn_edges_out = os.path.join(out_dir_local, \"edges_dynamic.parquet\")\n",
        "node_features.to_parquet(nodes_out, index=False)\n",
        "static_edges_df.to_parquet(static_edges_out, index=False)\n",
        "dyn_edges_df.to_parquet(dyn_edges_out, index=False)"
      ],
      "metadata": {
        "id": "HYnQXhhiY4SS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcs_prefix = PROCESSED_PREFIX + \"graph_dataset/\"\n",
        "for p in [nodes_out, static_edges_out, dyn_edges_out]:\n",
        "    bn = os.path.basename(p)\n",
        "    blob = bucket.blob(f\"{gcs_prefix}{bn}\")\n",
        "    with open(p,\"rb\") as f:\n",
        "        blob.upload_from_file(f)\n",
        "meta = {\"nodes\":nodes_out,\"edges_static\":static_edges_out,\"edges_dynamic\":dyn_edges_out,\"uploaded_at\":datetime.utcnow().isoformat()}\n",
        "meta_blob = bucket.blob(f\"{gcs_prefix}metadata.json\")\n",
        "meta_blob.upload_from_string(json.dumps(meta), content_type=\"application/json\")"
      ],
      "metadata": {
        "id": "W4YQp6wWY4Ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86443b64-f07f-4822-a8f9-a356518c6595"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3841086253.py:7: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  meta = {\"nodes\":nodes_out,\"edges_static\":static_edges_out,\"edges_dynamic\":dyn_edges_out,\"uploaded_at\":datetime.utcnow().isoformat()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_dates = sorted(node_features[\"date\"].dt.date.unique())\n",
        "for d in tqdm(unique_dates):\n",
        "    sub = node_features[node_features[\"date\"].dt.date==d].copy()\n",
        "    if sub.shape[0]==0:\n",
        "        continue\n",
        "    fn = f\"node_features_{d.strftime('%Y%m%d')}.parquet\"\n",
        "    localp = os.path.join(out_dir_local,fn)\n",
        "    sub.to_parquet(localp, index=False)\n",
        "    blob = bucket.blob(f\"{gcs_prefix}{fn}\")\n",
        "    with open(localp,\"rb\") as f:\n",
        "        blob.upload_from_file(f)"
      ],
      "metadata": {
        "id": "J9RNhXaqY-7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15b0efbd-bc46-497e-a449-4a02b3c79d48"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 733/733 [09:19<00:00,  1.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"done\")\n",
        "print(\"bucket\",BUCKET_NAME)\n",
        "print(\"processed prefix\",PROCESSED_PREFIX)\n",
        "print(\"latest aligned blob\", latest_blob.name)\n",
        "print(\"graph files uploaded to\", PROCESSED_PREFIX + \"graph_dataset/\")"
      ],
      "metadata": {
        "id": "ruFEh4nmY-5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638cf2d1-ffcf-4535-f661-bd6ececd5ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n",
            "bucket gdelt_raw_3_years\n",
            "processed prefix processed_data/\n",
            "latest aligned blob processed_data/final_aligned_data_20250908.json.gz\n",
            "graph files uploaded to processed_data/graph_dataset/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model: XGBoost"
      ],
      "metadata": {
        "id": "4YT_peZp17pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4L1H1N5B6tO",
        "outputId": "e51d3c22-d4e7-428a-8886-826ae3c3091b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from xgboost import XGBRegressor\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zwvfkp7G2IiE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeling_df = node_features.copy()\n",
        "target_col = 'wti_delta_next'\n",
        "modeling_df = modeling_df.dropna(subset=[target_col]).sort_values('date').reset_index(drop=True)"
      ],
      "metadata": {
        "id": "EC2QStHI2bej"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_all = modeling_df[feature_cols].fillna(0.0).values\n",
        "y_all = modeling_df[target_col].astype(float).values\n",
        "\n",
        "if modeling_df['date'].nunique() > 30:\n",
        "    cutoff_date = modeling_df['date'].max() - pd.Timedelta(days=90)\n",
        "    train_mask = modeling_df['date'] < cutoff_date\n",
        "    if train_mask.sum() == 0 or train_mask.sum() == len(modeling_df):\n",
        "        train_mask = np.arange(len(modeling_df)) < int(0.8 * len(modeling_df))\n",
        "else:\n",
        "    train_mask = np.arange(len(modeling_df)) < int(0.8 * len(modeling_df))"
      ],
      "metadata": {
        "id": "JtIVi6yJ6m9D"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = X_all[train_mask], X_all[~train_mask]\n",
        "y_train, y_test = y_all[train_mask], y_all[~train_mask]\n",
        "\n",
        "if len(y_test) == 0:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_all, y_all, test_size=0.2, random_state=42, shuffle=False\n",
        "    )"
      ],
      "metadata": {
        "id": "o8FRuNEj7Lo4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = XGBRegressor(\n",
        "    objective='reg:squarederror',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [200, 300, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "    'reg_lambda': [0.1, 1.0, 5.0]\n",
        "}"
      ],
      "metadata": {
        "id": "do9sspl7-6OU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# 4. Set up the GridSearchCV\n",
        "# We use 'r2' as the scoring metric to optimize for.\n",
        "# n_jobs=-1 uses all available CPU cores to speed up the search.\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='r2',\n",
        "    cv=tscv,  # <--- Use TimeSeriesSplit here!\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")"
      ],
      "metadata": {
        "id": "cwCUSWBc_Cxb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.fit(X_all, y_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf7VgM8T_Jye",
        "outputId": "37e0da47-cd83-4113-d635-eb2970f53777"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best R-squared score found: {grid_search.best_score_:.4f}\")\n",
        "print(\"Best parameters found:\")\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "QZPu4Uoezpgw",
        "outputId": "1ba98ee8-63bb-4400-d123-78bd779fe194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best R-squared score found: -0.0034\n",
            "Best parameters found:\n",
            "{'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'reg_lambda': 0.1, 'subsample': 0.7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "mDfOXu9L_Oyu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = best_model.predict(X_test)\n",
        "metrics = {\n",
        "    'MAE': mean_absolute_error(y_test, y_pred),\n",
        "    'RMSE': mean_squared_error(y_test, y_pred),\n",
        "    'R2': r2_score(y_test, y_pred)\n",
        "}\n",
        "print('Test metrics:')\n",
        "for name, value in metrics.items():\n",
        "    print(f'  {name}: {value:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnRdm_tu8ew4",
        "outputId": "2d9a6797-1aaa-42c2-9eed-6d69a7e6c0b4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test metrics:\n",
            "  MAE: 1.0686\n",
            "  RMSE: 2.2176\n",
            "  R2: -0.0030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNN Model"
      ],
      "metadata": {
        "id": "s9rgHaE69zoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqoVIXH5pCWq",
        "outputId": "7a849692-47ae-4a0c-f269-96d7e063b1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "NyPbB10u93Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_features['date'] = pd.to_datetime(node_features['date'])\n",
        "price_by_date.index = pd.to_datetime(price_by_date.index)\n",
        "if not dyn_edges_df.empty:\n",
        "    dyn_edges_df['date'] = pd.to_datetime(dyn_edges_df['date'], errors='coerce')\n",
        "    if 'timestamp' in dyn_edges_df.columns:\n",
        "        dyn_edges_df['timestamp'] = pd.to_datetime(dyn_edges_df['timestamp'], errors='coerce')\n",
        "    else:\n",
        "        dyn_edges_df['timestamp'] = dyn_edges_df['date']\n",
        "    dyn_edges_df['source_iso3'] = dyn_edges_df['source'].apply(to_iso3)\n",
        "    dyn_edges_df['target_iso3'] = dyn_edges_df['target'].apply(to_iso3)\n",
        "    dyn_edges_df = dyn_edges_df.dropna(subset=['source_iso3','target_iso3'])\n",
        "else:\n",
        "    dyn_edges_df['timestamp'] = pd.NaT\n",
        "    dyn_edges_df['source_iso3'] = []\n",
        "    dyn_edges_df['target_iso3'] = []"
      ],
      "metadata": {
        "id": "xJ4YznVWu_yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_columns_graph = [c for c in node_features.columns if c not in ['node_id','country','country_iso3','date','wti_price','brent_price','wti_delta_next','wti_ret_next']]\n",
        "feature_columns_graph = [c for c in feature_columns_graph if node_features[c].dtype != 'object']\n",
        "feature_columns_graph = sorted(dict.fromkeys(feature_columns_graph))\n",
        "\n",
        "market_indicator_candidates = []\n",
        "for col in node_features.columns:\n",
        "    low = col.lower()\n",
        "    if any(key in low for key in ['spread','usd','inventory','fx','dxy','dollar','inflation']):\n",
        "        if node_features[col].dtype != 'object':\n",
        "            market_indicator_candidates.append(col)\n",
        "market_indicator_cols = ['wti_price','brent_price'] + sorted(dict.fromkeys(market_indicator_candidates))"
      ],
      "metadata": {
        "id": "J9t7UT33955M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GraphSnapshot:\n",
        "    date: pd.Timestamp\n",
        "    nodes: pd.DataFrame\n",
        "    adjacency: np.ndarray\n",
        "    node_embeddings: np.ndarray\n",
        "    global_vector: np.ndarray\n",
        "    target: float\n",
        "\n",
        "def _normalize_adjacency(adj: np.ndarray) -> np.ndarray:\n",
        "    deg = adj.sum(axis=1, keepdims=True)\n",
        "    deg[deg == 0] = 1.0\n",
        "    return adj / deg\n",
        "\n",
        "def build_adjacency_matrix(nodes_df: pd.DataFrame, static_edges_df: pd.DataFrame, dyn_edges_df: pd.DataFrame, snapshot_time: pd.Timestamp, lookback_hours: int = 24) -> np.ndarray:\n",
        "    n = len(nodes_df)\n",
        "    adjacency = np.zeros((n, n), dtype=float)\n",
        "    iso_to_indices = defaultdict(list)\n",
        "    for idx, iso in enumerate(nodes_df['country_iso3']):\n",
        "        iso_to_indices[iso].append(idx)\n",
        "    if not static_edges_df.empty:\n",
        "        for _, edge in static_edges_df.iterrows():\n",
        "            src_nodes = iso_to_indices.get(edge['source'], [])\n",
        "            tgt_nodes = iso_to_indices.get(edge['target'], [])\n",
        "            for i in src_nodes:\n",
        "                for j in tgt_nodes:\n",
        "                    adjacency[i, j] = 1.0\n",
        "                    adjacency[j, i] = 1.0\n",
        "    if not dyn_edges_df.empty and snapshot_time is not None:\n",
        "        window_start = snapshot_time - pd.Timedelta(hours=lookback_hours)\n",
        "        mask = dyn_edges_df['timestamp'].between(window_start, snapshot_time)\n",
        "        for _, edge in dyn_edges_df.loc[mask].iterrows():\n",
        "            src_nodes = iso_to_indices.get(edge['source_iso3'], [])\n",
        "            tgt_nodes = iso_to_indices.get(edge['target_iso3'], [])\n",
        "            for i in src_nodes:\n",
        "                for j in tgt_nodes:\n",
        "                    adjacency[i, j] += 1.0\n",
        "                    adjacency[j, i] += 1.0\n",
        "    if n > 0:\n",
        "        np.fill_diagonal(adjacency, 1.0)\n",
        "    return adjacency\n",
        "\n",
        "def run_message_passing(nodes_df: pd.DataFrame, adjacency: np.ndarray, feature_columns: list, steps: int = 2) -> np.ndarray:\n",
        "    if len(nodes_df) == 0:\n",
        "        return np.empty((0, 0))\n",
        "    base_features = nodes_df[feature_columns].astype(float).to_numpy()\n",
        "    mean = np.nanmean(base_features, axis=0, keepdims=True)\n",
        "    std = np.nanstd(base_features, axis=0, keepdims=True)\n",
        "    std[std == 0] = 1.0\n",
        "    h = (base_features - mean) / std\n",
        "    norm_adj = _normalize_adjacency(adjacency)\n",
        "    for _ in range(steps):\n",
        "        neighbor_messages = norm_adj @ h\n",
        "        h = np.tanh(h + neighbor_messages)\n",
        "    embeddings = np.hstack([h, neighbor_messages])\n",
        "    embeddings = np.nan_to_num(embeddings, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return embeddings\n",
        "\n",
        "def attention_readout(nodes_df: pd.DataFrame, node_embeddings: np.ndarray, weight_keys: tuple = ('tone','tone_score','goldstein')) -> tuple:\n",
        "    if node_embeddings.size == 0:\n",
        "        return np.array([]), []\n",
        "    n_nodes = len(nodes_df)\n",
        "    scores = np.zeros(n_nodes, dtype=float)\n",
        "    for key in weight_keys:\n",
        "        candidates = [c for c in nodes_df.columns if key in c.lower() and nodes_df[c].dtype != 'object']\n",
        "        if candidates:\n",
        "            scores += nodes_df[candidates].astype(float).fillna(0.0).sum(axis=1).to_numpy()\n",
        "    if np.all(scores == 0):\n",
        "        scores = np.ones_like(scores)\n",
        "    scores = scores - scores.max()\n",
        "    weights = np.exp(scores)\n",
        "    weights = weights / weights.sum()\n",
        "    aggregated = weights @ node_embeddings\n",
        "    feature_names = [f\"gnn_embedding_{i}\" for i in range(node_embeddings.shape[1])]\n",
        "    return aggregated, feature_names\n",
        "\n",
        "def compute_market_features(nodes_df: pd.DataFrame, indicator_cols: list) -> tuple:\n",
        "    values = []\n",
        "    names = []\n",
        "    for col in indicator_cols:\n",
        "        if col not in nodes_df.columns:\n",
        "            continue\n",
        "        col_values = pd.to_numeric(nodes_df[col], errors='coerce')\n",
        "        values.extend([\n",
        "            np.nanmean(col_values.values),\n",
        "            np.nanstd(col_values.values)\n",
        "        ])\n",
        "        names.extend([f\"{col}_mean\", f\"{col}_std\"])\n",
        "    if 'wti_price' in nodes_df.columns:\n",
        "        latest_price = pd.to_numeric(nodes_df['wti_price'], errors='coerce').iloc[-1]\n",
        "        values.append(latest_price)\n",
        "        names.append('wti_price_current')\n",
        "    if 'brent_price' in nodes_df.columns:\n",
        "        latest_brent = pd.to_numeric(nodes_df['brent_price'], errors='coerce').iloc[-1]\n",
        "        values.append(latest_brent)\n",
        "        names.append('brent_price_current')\n",
        "    values = np.nan_to_num(np.array(values, dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return values, names\n",
        "\n",
        "def build_global_vector(snapshot_time: pd.Timestamp, nodes_df: pd.DataFrame, adjacency: np.ndarray) -> tuple:\n",
        "    node_embeddings = run_message_passing(nodes_df, adjacency, feature_columns_graph)\n",
        "    graph_vector, graph_names = attention_readout(nodes_df, node_embeddings)\n",
        "    market_vector, market_names = compute_market_features(nodes_df, market_indicator_cols)\n",
        "    combined = np.concatenate([graph_vector, market_vector])\n",
        "    feature_names = graph_names + market_names\n",
        "    return node_embeddings, combined, feature_names\n",
        "\n",
        "def build_snapshots(node_features: pd.DataFrame, static_edges_df: pd.DataFrame, dyn_edges_df: pd.DataFrame, price_by_date: pd.DataFrame, lookback_hours: int = 24) -> tuple:\n",
        "    rows = []\n",
        "    targets = []\n",
        "    dates = []\n",
        "    snapshot_records = []\n",
        "    feature_names = None\n",
        "    for ts in sorted(node_features['date'].unique()):\n",
        "        ts = pd.Timestamp(ts)\n",
        "        nodes_df = node_features[node_features['date'] == ts].copy()\n",
        "        if nodes_df.empty:\n",
        "            continue\n",
        "        adjacency = build_adjacency_matrix(nodes_df, static_edges_df, dyn_edges_df, ts, lookback_hours=lookback_hours)\n",
        "        node_embeddings, global_vector, names = build_global_vector(ts, nodes_df, adjacency)\n",
        "        target = price_by_date['wti_ret_next'].reindex([ts]).iloc[0] if ts in price_by_date.index else np.nan\n",
        "        if pd.isna(target):\n",
        "            continue\n",
        "        rows.append(global_vector)\n",
        "        targets.append(float(target))\n",
        "        dates.append(ts)\n",
        "        snapshot_records.append(GraphSnapshot(ts, nodes_df, adjacency, node_embeddings, global_vector, float(target)))\n",
        "        if feature_names is None:\n",
        "            feature_names = names\n",
        "    if not rows:\n",
        "        raise ValueError('No graph snapshots were built; verify that node features and price targets overlap.')\n",
        "    data_matrix = np.vstack(rows)\n",
        "    targets = np.array(targets, dtype=float)\n",
        "    return data_matrix, targets, dates, feature_names, snapshot_records"
      ],
      "metadata": {
        "id": "ockAC-Sg-KdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionStump:\n",
        "      def __init__(self, max_thresholds: int = 32):\n",
        "          self.max_thresholds = max_thresholds\n",
        "          self.feature_index = 0\n",
        "          self.threshold = 0.0\n",
        "          self.left_value = 0.0\n",
        "          self.right_value = 0.0\n",
        "\n",
        "      def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "          n_samples, n_features = X.shape\n",
        "          best_error = np.inf\n",
        "          overall_mean = y.mean() if len(y) > 0 else 0.0\n",
        "          if n_samples == 0:\n",
        "              self.left_value = overall_mean\n",
        "              self.right_value = overall_mean\n",
        "              return\n",
        "          for feature in range(n_features):\n",
        "              column = X[:, feature]\n",
        "              unique_vals = np.unique(column)\n",
        "              if len(unique_vals) > self.max_thresholds:\n",
        "                  thresholds = np.linspace(unique_vals.min(), unique_vals.max(), self.max_thresholds)\n",
        "              else:\n",
        "                  thresholds = unique_vals\n",
        "              for thr in thresholds:\n",
        "                  left_mask = column <= thr\n",
        "                  right_mask = ~left_mask\n",
        "                  if left_mask.sum() == 0 or right_mask.sum() == 0:\n",
        "                      continue\n",
        "                  left_value = y[left_mask].mean()\n",
        "                  right_value = y[right_mask].mean()\n",
        "                  predictions = np.where(left_mask, left_value, right_value)\n",
        "                  error = np.mean((y - predictions) ** 2)\n",
        "                  if error < best_error:\n",
        "                      best_error = error\n",
        "                      self.feature_index = feature\n",
        "                      self.threshold = thr\n",
        "                      self.left_value = left_value\n",
        "                      self.right_value = right_value\n",
        "          if best_error == np.inf:\n",
        "              self.left_value = overall_mean\n",
        "              self.right_value = overall_mean\n",
        "\n",
        "      def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "          if X.size == 0:\n",
        "              return np.array([])\n",
        "          mask = X[:, self.feature_index] <= self.threshold\n",
        "          return np.where(mask, self.left_value, self.right_value)"
      ],
      "metadata": {
        "id": "nHRDYCFvi4Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBoostedStumps:\n",
        "      def __init__(self, n_estimators: int = 200, learning_rate: float = 0.05, max_thresholds: int = 32):\n",
        "          self.n_estimators = n_estimators\n",
        "          self.learning_rate = learning_rate\n",
        "          self.max_thresholds = max_thresholds\n",
        "          self.estimators_ = []\n",
        "          self.init_value_ = 0.0\n",
        "          self.feature_counts_ = None\n",
        "\n",
        "      def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "          self.estimators_ = []\n",
        "          self.feature_counts_ = np.zeros(X.shape[1], dtype=float)\n",
        "          self.init_value_ = y.mean() if len(y) > 0 else 0.0\n",
        "          y_pred = np.full_like(y, self.init_value_, dtype=float)\n",
        "          for _ in range(self.n_estimators):\n",
        "              residual = y - y_pred\n",
        "              stump = DecisionStump(max_thresholds=self.max_thresholds)\n",
        "              stump.fit(X, residual)\n",
        "              update = stump.predict(X)\n",
        "              y_pred += self.learning_rate * update\n",
        "              self.estimators_.append(stump)\n",
        "              self.feature_counts_[stump.feature_index] += 1\n",
        "\n",
        "      def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "          if X.size == 0:\n",
        "              return np.array([])\n",
        "          y_pred = np.full(X.shape[0], self.init_value_, dtype=float)\n",
        "          for stump in self.estimators_:\n",
        "              y_pred += self.learning_rate * stump.predict(X)\n",
        "          return y_pred\n",
        "\n",
        "      def feature_importances(self) -> np.ndarray:\n",
        "          if self.feature_counts_ is None:\n",
        "              return np.array([])\n",
        "          total = self.feature_counts_.sum()\n",
        "          if total == 0:\n",
        "              return np.zeros_like(self.feature_counts_)\n",
        "          return self.feature_counts_ / total"
      ],
      "metadata": {
        "id": "4-twdQcPjvBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gnn_lgbm_pipeline(node_features: pd.DataFrame, static_edges_df: pd.DataFrame, dyn_edges_df: pd.DataFrame, price_by_date: pd.DataFrame, lookback_hours: int = 24, test_fraction: float = 0.2):\n",
        "      X, y, dates, feature_names, snapshots = build_snapshots(node_features, static_edges_df, dyn_edges_df, price_by_date, lookback_hours=lookback_hours)\n",
        "      n_samples = len(X)\n",
        "      split_index = max(1, int(n_samples * (1 - test_fraction)))\n",
        "      X_train, X_test = X[:split_index], X[split_index:]\n",
        "      y_train, y_test = y[:split_index], y[split_index:]\n",
        "      dates_train, dates_test = dates[:split_index], dates[split_index:]\n",
        "\n",
        "      model = GradientBoostedStumps(n_estimators=200, learning_rate=0.05, max_thresholds=32)\n",
        "      model.fit(X_train, y_train)\n",
        "      train_pred = model.predict(X_train)\n",
        "      test_pred = model.predict(X_test) if len(X_test) > 0 else np.array([])\n",
        "\n",
        "      def mae(targets, preds):\n",
        "          return float(np.mean(np.abs(targets - preds))) if len(targets) else float('nan')\n",
        "\n",
        "      def rmse(targets, preds):\n",
        "          return float(np.sqrt(np.mean((targets - preds) ** 2))) if len(targets) else float('nan')\n",
        "\n",
        "      def directional_accuracy(targets, preds):\n",
        "          if len(targets) == 0:\n",
        "              return float('nan')\n",
        "          mask = (targets != 0) & (preds != 0)\n",
        "          if mask.sum() == 0:\n",
        "              return float('nan')\n",
        "          return float(np.mean(np.sign(targets[mask]) == np.sign(preds[mask])))\n",
        "\n",
        "      metrics = {\n",
        "          'train_mae': mae(y_train, train_pred),\n",
        "          'train_rmse': rmse(y_train, train_pred),\n",
        "          'train_directional_accuracy': directional_accuracy(y_train, train_pred),\n",
        "          'test_mae': mae(y_test, test_pred),\n",
        "          'test_rmse': rmse(y_test, test_pred),\n",
        "          'test_directional_accuracy': directional_accuracy(y_test, test_pred)\n",
        "      }\n",
        "\n",
        "      results = {\n",
        "          'model': model,\n",
        "          'feature_names': feature_names,\n",
        "          'metrics': metrics,\n",
        "          'train_predictions': pd.DataFrame({'date': dates_train, 'target': y_train, 'prediction': train_pred}),\n",
        "          'test_predictions': pd.DataFrame({'date': dates_test, 'target': y_test, 'prediction': test_pred}),\n",
        "          'snapshots': snapshots\n",
        "      }\n",
        "      return results"
      ],
      "metadata": {
        "id": "GkCoRjSVlcE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_results = train_gnn_lgbm_pipeline(\n",
        "    node_features=node_features,\n",
        "    static_edges_df=static_edges_df,\n",
        "    dyn_edges_df=dyn_edges_df,\n",
        "    price_by_date=price_by_date,\n",
        "    lookback_hours=24,\n",
        "    test_fraction=0.2\n",
        ")\n",
        "\n",
        "metrics_table = pd.DataFrame([pipeline_results['metrics']])\n",
        "metrics_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "bdNYAu31s1gM",
        "outputId": "1e422cf4-39c7-41ec-b1be-0fd417ef418b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   train_mae  train_rmse  train_directional_accuracy  test_mae  test_rmse  \\\n",
              "0   0.015557    0.019464                    0.605489  0.017943   0.023429   \n",
              "\n",
              "   test_directional_accuracy  \n",
              "0                   0.517007  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08077ce6-592c-4681-b029-5d6b7111b905\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>train_mae</th>\n",
              "      <th>train_rmse</th>\n",
              "      <th>train_directional_accuracy</th>\n",
              "      <th>test_mae</th>\n",
              "      <th>test_rmse</th>\n",
              "      <th>test_directional_accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.015557</td>\n",
              "      <td>0.019464</td>\n",
              "      <td>0.605489</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.023429</td>\n",
              "      <td>0.517007</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08077ce6-592c-4681-b029-5d6b7111b905')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08077ce6-592c-4681-b029-5d6b7111b905 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08077ce6-592c-4681-b029-5d6b7111b905');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_ddd869ba-1cb4-4924-9aa8-336acd636662\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_table')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ddd869ba-1cb4-4924-9aa8-336acd636662 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_table');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_table",
              "summary": "{\n  \"name\": \"metrics_table\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"train_mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.01555682514854497,\n        \"max\": 0.01555682514854497,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01555682514854497\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.019464331533395926,\n        \"max\": 0.019464331533395926,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.019464331533395926\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_directional_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6054888507718696,\n        \"max\": 0.6054888507718696,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6054888507718696\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_mae\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.01794312054827527,\n        \"max\": 0.01794312054827527,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01794312054827527\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.02342884346434165,\n        \"max\": 0.02342884346434165,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.02342884346434165\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_directional_accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5170068027210885,\n        \"max\": 0.5170068027210885,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5170068027210885\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CELL TO SAVE MODEL ARTIFACTS TO GCS ---\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "\n",
        "print(\"Saving model and evaluation data to GCS...\")\n",
        "\n",
        "# --- 1. Define Paths ---\n",
        "# Use a temporary local directory for staging files\n",
        "local_tmp_dir = \"/tmp/model_export\"\n",
        "os.makedirs(local_tmp_dir, exist_ok=True)\n",
        "\n",
        "# Define the GCS prefix where artifacts will be stored\n",
        "gcs_model_prefix = PROCESSED_PREFIX + \"model_artifacts/\"\n",
        "\n",
        "# Define local paths\n",
        "local_model_path = os.path.join(local_tmp_dir, \"gbs_model.joblib\")\n",
        "local_x_test_path = os.path.join(local_tmp_dir, \"X_test.npy\")\n",
        "local_y_test_path = os.path.join(local_tmp_dir, \"y_test.npy\")\n",
        "local_features_path = os.path.join(local_tmp_dir, \"feature_names.json\")\n",
        "\n",
        "# --- 2. Retrieve Data to Save ---\n",
        "model_to_save = pipeline_results['model']\n",
        "feature_names_to_save = pipeline_results['feature_names']\n",
        "\n",
        "# Re-build snapshots to get the correct test set split\n",
        "X_all, y_all, dates_all, _, _ = build_snapshots(\n",
        "    node_features, static_edges_df, dyn_edges_df, price_by_date, lookback_hours=24\n",
        ")\n",
        "n_samples = len(X_all)\n",
        "split_index = max(1, int(n_samples * (1 - 0.2)))\n",
        "X_test = X_all[split_index:]\n",
        "y_test = y_all[split_index:]\n",
        "\n",
        "\n",
        "# --- 3. Save Files Locally ---\n",
        "print(f\"Staging files in {local_tmp_dir}...\")\n",
        "# 1. Save the model\n",
        "joblib.dump(model_to_save, local_model_path)\n",
        "\n",
        "# 2. Save the test data\n",
        "np.save(local_x_test_path, X_test)\n",
        "np.save(local_y_test_path, y_test)\n",
        "\n",
        "# 3. Save the feature names\n",
        "with open(local_features_path, 'w') as f:\n",
        "    json.dump(feature_names_to_save, f)\n",
        "\n",
        "\n",
        "# --- 4. Upload Files to GCS ---\n",
        "print(f\"Uploading artifacts to GCS bucket '{BUCKET_NAME}' at prefix '{gcs_model_prefix}'...\")\n",
        "\n",
        "files_to_upload = [\n",
        "    (\"gbs_model.joblib\", local_model_path),\n",
        "    (\"X_test.npy\", local_x_test_path),\n",
        "    (\"y_test.npy\", local_y_test_path),\n",
        "    (\"feature_names.json\", local_features_path)\n",
        "]\n",
        "\n",
        "gcs_paths = {}\n",
        "\n",
        "for filename, local_path in files_to_upload:\n",
        "    try:\n",
        "        gcs_path = f\"{gcs_model_prefix}{filename}\"\n",
        "        blob = bucket.blob(gcs_path)\n",
        "        blob.upload_from_filename(local_path)\n",
        "        gcs_paths[filename] = f\"gs://{BUCKET_NAME}/{gcs_path}\"\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR uploading {filename}: {e}\")\n",
        "\n",
        "print(\"\\n--- Upload Complete ---\")\n",
        "print(f\"Artifacts saved to GCS:\")\n",
        "print(f\"  - Model: {gcs_paths.get('gbs_model.joblib')}\")\n",
        "print(f\"  - X_test: {gcs_paths.get('X_test.npy')} (shape: {X_test.shape})\")\n",
        "print(f\"  - y_test: {gcs_paths.get('y_test.npy')} (shape: {y_test.shape})\")\n",
        "print(f\"  - Features: {gcs_paths.get('feature_names.json')} (count: {len(feature_names_to_save)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0j1TMNmhjNC",
        "outputId": "08fee126-67a9-45ca-80eb-a615fdc7a8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model and evaluation data to GCS...\n",
            "Staging files in /tmp/model_export...\n",
            "Uploading artifacts to GCS bucket 'gdelt_raw_3_years' at prefix 'processed_data/model_artifacts/'...\n",
            "\n",
            "--- Upload Complete ---\n",
            "Artifacts saved to GCS:\n",
            "  - Model: gs://gdelt_raw_3_years/processed_data/model_artifacts/gbs_model.joblib\n",
            "  - X_test: gs://gdelt_raw_3_years/processed_data/model_artifacts/X_test.npy (shape: (147, 24))\n",
            "  - y_test: gs://gdelt_raw_3_years/processed_data/model_artifacts/y_test.npy (shape: (147,))\n",
            "  - Features: gs://gdelt_raw_3_years/processed_data/model_artifacts/feature_names.json (count: 24)\n",
            "\n",
            "Cleaned up temporary directory: /tmp/model_export\n"
          ]
        }
      ]
    }
  ]
}